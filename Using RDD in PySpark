# Open Jupyter Notebook Via Docker Module 

'''
RDD - Resilient Distributed Dataset If something goes down the data is saved because the data
is replicated.
RDD is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning, once you create an RDD, you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster (Divide and Conquer algorithm)

RDD Benefits
In-memory Processing: PySpark loads the data from disk and process into memory and keeps the data in memory
Immutability: Once you create an RDD, you cannot modify it. If you make any changes to the RDD, PySpark will create a new RDD and maintains the RDD lineage.
Fault-Tolerance: This is acheived because spark runs on a cluster
Partitioning: Example: If you have a records of anything from 1990-2021, you can partition this data for before 2020 and after 2021
Create RDD using parrallelize() function

'''

#Import Modules and PySpark

from pyspark.sql import SparkSession # This is the entry point for spark to work, with RDD, DataFrame, and Dataset
#Spark session provides API for 
#Spark Context
#SQL Context
#Streaming Context
#Hive Context

#Spark session & context
spark = SparkSession.builder.master('local').getOrCreate()
sc = spark.sparkContext

#Spark Context is an entry point for your Spark Package tools and libraries and used programatically to create
#-- Spark RDD, accumulators and broadcast variables on the cluster. 

# YOU CAN CREATE ONLY ONE SPARKCONTEXT PER JVM (JAVA VIRTUAL MACHINE)

data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
rdd = sc.parallelize(data)

# Create an empty RDD uning the emptyRDD Allows you to populate data at a later state
rddEmpty = sc.emptyRDD

# Create an empty RDD with Partition 
rddPartition = sc.parallelize([], 10) # the 10 creates 10 partitions 

rdd.collect() # This is not supposed to be used in production code; this must be used for debugging only
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]

rdd.first()
# output: 1

rdd.take(2) #first two elements
#output [1, 2]

rdd.getNumPartitions()
#output: 1

rddPartition_2 = sc.parallelize(data, 10)
rddPartition_2.getNumPartitions() 
#Output 10

#example on how to use this stuff 

tempData = [59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]
parTempData = sc.parallelize(tempData, 2) 
parTempData.collect()

tempdata = [59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]
parTempData = sc.parallelize(tempdata, 2) 
parTempData.collect()

[59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]

#Putting it into Practice

# Write a function to calculate Farenheit to C
def fahrenheitToCentigrade (temperature):
    centigrade = (temperature-32) * 5/9
    return centigrade

fahrenheitToCentigrade(59)
#output 15.0

parCentigradeData = parTempData.map(fahrenheitToCentigrade)
parCentigradeData.collect()
#output [15.0, 14.000000000000002, 12.0, 13.0, 10.999999999999998, 12.0, 13.0]


# how to filter in your RDD Data - You can use filter
#define a predicate - predicate is a function that tests a condition and returns true or false
#define a predicate
def tempMoreThanTirteen(temperature):
    return temperature >= 13
filteredTemperatureRDD = parCentigradeData.filter(tempMoreThanTirteen)
filteredTemperatureRDD.collect()
#output [15.0, 14.000000000000002, 13.0, 13.0]

filteredTemperatureRDD = parCentigradeData.filter(lambda x: x>= 13)
filteredTemperatureRDD.collect()
[15.0, 14.000000000000002, 13.0, 13.0]

'''
Perform Basic Data Manipulation
Using a student databse, Please perform the following objectives
Average grades per semester, each year, for each student
Top three students who have the highest average grades in the school year
Bottom three students who have the lowest average grades in the second year
All students who have earned more than an 80% average in the second semester 
'''

studentMarksData = [["si1","year1",62.08,62.4],
    ["si1","year2",75.94,76.75],
    ["si2","year1",68.26,72.95],
    ["si2","year2",85.49,75.8],
    ["si3","year1",75.08,79.84],
    ["si3","year2",54.98,87.72],
    ["si4","year1",50.03,66.85],
    ["si4","year2",71.26,69.77],
    ["si5","year1",52.74,76.27],
    ["si5","year2",50.39,68.58],
    ["si6","year1",74.86,60.8],
    ["si6","year2",58.29,62.38],
    ["si7","year1",63.95,74.51],
    ["si7","year2",66.69,56.92]]
#Parallelize the data
studentMarksDataRDD = sc.parallelize(studentMarksData, 4)
studentMarksDataRDD.take(2) 

#Objective 1: Calculate Average Semester Grades
#In the map function, and for lambda, the value of x is the first collection from the list
#Example: x = 
studentMarksMean = studentMarksDataRDD.map(lambda x: [x[0], x[1], (x[2]+x[3])/2])
studentMarksMean.take(2)
[['si1', 'year1', 62.239999999999995], ['si1', 'year2', 76.345]]

#Filtering Student Average Grades in the second year
secondYearMarks = studentMarksMean.filter(lambda x: "year2" in x)
secondYearMarks.take(2) 
[['si1', 'year2', 76.345], ['si2', 'year2', 80.645]]

# Finding the Top Three Students in second years
sortedMarksData = secondYearMarks.sortBy(keyfunc = lambda x: -x[2])
sortedMarksData.collect()
[['si2', 'year2', 80.645],
 ['si1', 'year2', 76.345],
 ['si3', 'year2', 71.35],
 ['si4', 'year2', 70.515],
 ['si7', 'year2', 61.805],
 ['si6', 'year2', 60.335],
 ['si5', 'year2', 59.485]]
 
sortedMarksData.take(3)
[['si2', 'year2', 80.645], ['si1', 'year2', 76.345], ['si3', 'year2', 71.35]]
#Take ordered Function 
topThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x : -x[2] )
print(topThreeStudents)
[['si2', 'year2', 80.645], ['si1', 'year2', 76.345], ['si3', 'year2', 71.35]]
#Print bottom three students
botThreeStudents = secondYearMarks.takeOrdered(num=3, key = lambda x: x[2])
print(botThreeStudents) 
[['si5', 'year2', 59.485], ['si6', 'year2', 60.335], ['si7', 'year2', 61.805]]
#Get all students 80 or above


#Create a new RDD dataset for students who have more than 80 marks
moreThan80Marks = secondYearMarks.filter(lambda x : x[2] >= 80)
moreThan80Marks.collect()
[['si2', 'year2', 80.645]]
